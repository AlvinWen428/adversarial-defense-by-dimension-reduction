{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import queue\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from locally_linear import LocallyLinearBackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "memory_gpu=[int(x.split()[2]) for x in open('tmp','r').readlines()]\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(np.argmax(memory_gpu))\n",
    "os.system('rm tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 1\n",
    "n_dimentions = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(28),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "data_train = dsets.MNIST(root = \"./data/\",\n",
    "                         transform=transform,\n",
    "                            train = True,\n",
    "                            download = True)\n",
    "\n",
    "data_test = dsets.MNIST(root=\"./data/\",\n",
    "                        transform=transform,\n",
    "                           train = False)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=data_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(dataset=data_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_conv(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_conv, self).__init__()\n",
    "        self.layer1 = tnn.Sequential(\n",
    "\n",
    "            # 1-1 conv layer\n",
    "            tnn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(64),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 1-2 conv layer\n",
    "            tnn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(64),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 1 Pooling layer\n",
    "            tnn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = tnn.Sequential(\n",
    "\n",
    "            # 2-1 conv layer\n",
    "            tnn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(128),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 2-2 conv layer\n",
    "            tnn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(128),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 2 Pooling lyaer\n",
    "            tnn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = tnn.Sequential(\n",
    "\n",
    "            # 3-1 conv layer\n",
    "            tnn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(256),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 3-2 conv layer\n",
    "            tnn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(256),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 3 Pooling layer\n",
    "            tnn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer4 = tnn.Sequential(\n",
    "\n",
    "            # 4-1 conv layer\n",
    "            tnn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(512),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 4-2 conv layer\n",
    "            tnn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            tnn.BatchNorm2d(512),\n",
    "            tnn.ReLU(),\n",
    "\n",
    "            # 4 Pooling layer\n",
    "            tnn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # self.layer5 = tnn.Sequential(\n",
    "        #\n",
    "        #     # 5-1 conv layer\n",
    "        #     tnn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        #     tnn.BatchNorm2d(512),\n",
    "        #     tnn.ReLU(),\n",
    "        #\n",
    "        #     # 5-2 conv layer\n",
    "        #     tnn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        #     tnn.BatchNorm2d(512),\n",
    "        #     tnn.ReLU(),\n",
    "        #\n",
    "        #     # 5 Pooling layer\n",
    "        #    tnn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer6 = tnn.Sequential(\n",
    "\n",
    "            # 6 Fully connected layer\n",
    "            # Dropout layer omitted since batch normalization is used.\n",
    "            tnn.Linear(512, 512),\n",
    "            tnn.BatchNorm1d(512),\n",
    "            tnn.ReLU())\n",
    "\n",
    "\n",
    "        self.layer7 = tnn.Sequential(\n",
    "\n",
    "            # 7 Fully connected layer\n",
    "            # Dropout layer omitted since batch normalization is used.\n",
    "            tnn.Linear(512, 512,\n",
    "            tnn.BatchNorm1d(512)),\n",
    "            tnn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "      out = self.layer1(x)\n",
    "      out = self.layer2(out)\n",
    "      out = self.layer3(out)\n",
    "      out = self.layer4(out)\n",
    "   #   out = self.layer5(out)\n",
    "      vgg16_features = out.view(out.size(0), -1)\n",
    "      out = self.layer6(vgg16_features)\n",
    "      out = self.layer7(out)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_fc(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_fc, self).__init__()\n",
    "        self.layer8 = tnn.Sequential(\n",
    "\n",
    "        # 8 output layer\n",
    "        tnn.Linear(32, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer8(x)\n",
    "#         out = F.softmax(out, dim=1)  #CrossEntropy 不能用这个\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isomap(feature_queue, n_components):\n",
    "    length = feature_queue.qsize()\n",
    "    for i in range(length):\n",
    "        if i == 0:\n",
    "            feature_tmp = feature_queue.get()\n",
    "            feature_queue.put(feature_tmp)\n",
    "            features = feature_tmp\n",
    "            feature_to_use = feature_tmp\n",
    "        else:\n",
    "            feature_tmp = feature_queue.get()\n",
    "            feature_queue.put(feature_tmp)\n",
    "            features = torch.cat((features, feature_tmp), dim=0)\n",
    "        \n",
    "    feature_input = features.detach().cpu().numpy()\n",
    "    embedding = Isomap(n_components=n_components)\n",
    "    transformed = embedding.fit_transform(feature_input)\n",
    "    if features.is_cuda:\n",
    "        output = torch.from_numpy(transformed).cuda()\n",
    "    else:\n",
    "        output = torch.from_numpy(transformed)\n",
    "    return output, feature_to_use\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_conv = VGG_conv()\n",
    "# vgg_conv.cuda()\n",
    "vgg_fc = VGG_fc()\n",
    "# vgg_fc.cuda()\n",
    "\n",
    "isomap_feature = torch.empty(BATCH_SIZE, n_dimentions, requires_grad=True) \n",
    "cost1 = tnn.CosineEmbeddingLoss()\n",
    "cost2 = tnn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(vgg_conv.parameters(), lr=LEARNING_RATE)\n",
    "optimizer2 = torch.optim.Adam([{'params':vgg_fc.parameters()},{'params':isomap_feature}], lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 32])\n",
      "torch.Size([50, 32])\n",
      "torch.Size([50, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6bf72413aec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0moptimizer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0moptimizer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m#----------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(EPOCH):\n",
    "#  for i, (images, labels) in enumerate(trainLoader):\n",
    "  vgg_conv.train()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  train_img_queue = queue.Queue(maxsize=1000/BATCH_SIZE)    #构建输入图像的队列\n",
    "  train_label_queue = queue.Queue(maxsize=1000/BATCH_SIZE) #构建label的队列\n",
    "  train_vec_queue = queue.Queue(maxsize=1000/BATCH_SIZE)    #构建卷积网络输出向量的队列\n",
    "  for images, labels in trainLoader:\n",
    "    train_img_queue.put(images)\n",
    "    train_label_queue.put(labels)\n",
    "    \n",
    "    # Forward + Backward + Optimize\n",
    "    \n",
    "#     optimizer1.zero_grad()\n",
    "#     optimizer2.zero_grad()\n",
    "\n",
    "    outputs1 = vgg_conv(images) #卷积网络的输出，将图片embedding成512维向量，the shape of output is (batch_size, 512)\n",
    "    \n",
    "    train_vec_queue.put(outputs1)\n",
    "    \n",
    "#     print(train_vec_queue.qsize())\n",
    "#     print(train_vec_queue.get_nowait().shape)\n",
    "    \n",
    "    if train_img_queue.full():  #等队列满了之后，开始让所有图片进入isomap，然后pop出队首的数据进行反向传播\n",
    "        isomap_forward, feature_to_use = isomap(train_vec_queue, n_components=32)#将1000张图片通过卷积层得到的embedding向量输入isomap层，获得降维后的结果\n",
    "        batch_feature = isomap_forward[:BATCH_SIZE].float()\n",
    "        \n",
    "        img_tmp = train_img_queue.get()\n",
    "        label_tmp = train_label_queue.get()\n",
    "        vec_tmp = train_vec_queue.get()\n",
    "        \n",
    "        print(isomap_forward.shape)\n",
    "        print(batch_feature.shape)\n",
    "        \n",
    "        isomap_feature = batch_feature\n",
    "        outputs = vgg_fc(isomap_feature)\n",
    "        \n",
    "        print(outputs.shape)\n",
    "        loss2 = cost2(outputs, labels)\n",
    "        \n",
    "        #---------------------------------------\n",
    "        Y = isomap_feature.detach().cpu().numpy()\n",
    "        \n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        Y_hat = isomap_feature.detach().cpu().numpy()\n",
    "        \n",
    "        back = LocallyLinearBackward(n_neighbors=10) # n_neighbors is a hyperparameter\n",
    "        back.fit(Y, Y_hat)\n",
    "        \n",
    "        X_hat = back.error_backward(feature_to_use.detach().cpu().numpy())\n",
    "        target = torch.from_numpy(X_hat)\n",
    "        loss1 = cost1(target, feature_to_use)\n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        #----------------------------------------\n",
    "#         current_img = train_img_queue.get()\n",
    "#         current_label = train_label_queue.get()\n",
    "#         current_vec = train_vec_queue.get()\n",
    "        \n",
    "    \n",
    "    \n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "#     loss = cost(outputs, labels.cuda())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#   print ('Epoch [%d/%d], Loss. %.4f' %\n",
    "#              (epoch+1, EPOCH, loss.data[0]))\n",
    "#   print('Test Accuracy of the model on the training set: %d %%' % (100 * correct / total))\n",
    "\n",
    "# # Test the model\n",
    "#   vgg16.eval()\n",
    "#   correct = 0\n",
    "#   total = 0\n",
    "\n",
    "#   for images, labels in testLoader:\n",
    "#     images = Variable(images).cuda()\n",
    "#     outputs = vgg16(images)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "#   print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# # Save the Trained Model\n",
    "# torch.save(vgg16.state_dict(),'checkpoint_without_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel = VGG_fc()\n",
    "# input dimention = 32\n",
    "Input = torch.ones(50,32)\n",
    "Input.requires_grad = True\n",
    "optimizer = torch.optim.Adam([{'params':testModel.parameters()},\n",
    "                           {'params':Input}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = testModel(Input)\n",
    "\n",
    "cost = tnn.CrossEntropyLoss()\n",
    "loss = cost(output, torch.ones(50).long())\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0011, -0.0017,  0.0018,  ...,  0.0016,  0.0015,  0.0004],\n",
       "        [-0.0011, -0.0017,  0.0018,  ...,  0.0016,  0.0015,  0.0004],\n",
       "        [-0.0011, -0.0017,  0.0018,  ...,  0.0016,  0.0015,  0.0004],\n",
       "        ...,\n",
       "        [-0.0011, -0.0017,  0.0018,  ...,  0.0016,  0.0015,  0.0004],\n",
       "        [-0.0011, -0.0017,  0.0018,  ...,  0.0016,  0.0015,  0.0004],\n",
       "        [-0.0011, -0.0017,  0.0018,  ...,  0.0016,  0.0015,  0.0004]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input = Input *0 + torch.ones((50,32))\n",
    "Input.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = torch.from_numpy(np.ones((50,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Input.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
